\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces \cite {Levoy96}.}}{5}
\contentsline {figure}{\numberline {1.2}{\ignorespaces }}{6}
\contentsline {figure}{\numberline {1.3}{\ignorespaces }}{6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The plenoptic function sampled at two points, showing all pencils of light rays which pass through those points \cite {Adelson91}.}}{8}
\contentsline {figure}{\numberline {2.2}{\ignorespaces This image depicts the most common parameterizations at the time ***cite was written. Of the three, the two plane parameterization on the far right persists as the most popular.}}{10}
\contentsline {figure}{\numberline {2.3}{\ignorespaces The dense camera array used at Stanford to capture a number of light fields.}}{13}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Another collection method, the gantry effectively mimics the camera array so long as the scene is not altered between gantry locations.}}{13}
\contentsline {figure}{\numberline {2.5}{\ignorespaces the lytro immerge, designed to capture an entire scene at once, with an array of cameras built into the structure.}}{14}
\contentsline {figure}{\numberline {2.6}{\ignorespaces On the right is the camera rig behind the Google Jump VR video system \cite {Anderson16}}}{14}
\contentsline {figure}{\numberline {2.7}{\ignorespaces the lytro immerge, designed to capture an entire scene at once, with an array of cameras built into the structure.}}{15}
\contentsline {figure}{\numberline {2.8}{\ignorespaces On the right is the camera rig behind the Google Jump VR video system \cite {Anderson16}}}{15}
\contentsline {figure}{\numberline {2.9}{\ignorespaces This image depicts the most common parameterizations at the time ***cite was written. Of the three, the two plane parameterization on the far right persists as the most popular.}}{16}
\contentsline {figure}{\numberline {2.10}{\ignorespaces This image depicts the most common parameterizations at the time ***cite was written. Of the three, the two plane parameterization on the far right persists as the most popular.}}{17}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces An image of the baxter robot used to acquire some of the first data sets.}}{20}
\contentsline {figure}{\numberline {3.2}{\ignorespaces In this figure, the variance for the different methods, ie Baxter and PTAM are shown. Baxter is the blue column, and PTAM is the black. Clearly, in terms of localization, Baxter is superior to PTAM.}}{21}
\contentsline {figure}{\numberline {3.3}{\ignorespaces This figure shows the most complex part of the experiment, where PTAM has been initialized, and Baxter is running through its image collection program.}}{22}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Shows the location of the laser (on the left) and where Baxter estimates its position to be (right).}}{23}
\contentsline {figure}{\numberline {3.5}{\ignorespaces The second robot platform tested is the PR2. My hypothesis going into testing with it is that it will be considerably more accurate and precise than the Baxter.}}{24}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Shows the location of the laser (on the left) and where PR2 estimates its position to be (right).}}{25}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Shows the location of the laser (on the left) and where ORB-SLAM2 estimates its position to be (right).}}{26}
\contentsline {figure}{\numberline {3.8}{\ignorespaces An accuracy comparison between the Baxter and PR2.}}{26}
\contentsline {figure}{\numberline {3.9}{\ignorespaces This mosaic is a compilation of all the images taken during one light field acquisition session with the baxter robot.}}{27}
\contentsline {figure}{\numberline {3.10}{\ignorespaces A depiction of the second renderer based on \cite {Isaksen01} and showing the plane of focus $F$ and the rays coming from it that would be considered in focus.}}{27}
\contentsline {figure}{\numberline {3.11}{\ignorespaces A depiction of the second renderer based on \cite {Isaksen01} and showing the plane of focus $F$ and the rays coming from it that would be considered in focus.}}{28}
\contentsline {figure}{\numberline {3.12}{\ignorespaces A screenshot of the light field interface in action, with the subaperture views shown in blue, the aperture shown as the red circle, and the resulting image shown in the larger window.}}{28}
\contentsline {figure}{\numberline {3.13}{\ignorespaces The dense camera array used at Stanford to capture a number of light fields.}}{29}
\contentsline {figure}{\numberline {3.14}{\ignorespaces Another collection method, the gantry effectively mimics the camera array so long}}{29}
\contentsline {figure}{\numberline {3.15}{\ignorespaces Image of a Nau posing for the spherical parameterization collection. Individual images are shown in the sub-aperture view and are denoted by the blue squares.}}{30}
\contentsline {figure}{\numberline {3.16}{\ignorespaces The most recent version of the rendering algorithm where the robot moves continuously and captures very dense data. Shown in the sub-aperture view is approximately half of the actual captured views, with the other half having been taken out in a uniformly random pattern for testing.}}{31}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces An initial view of the rays intersecting a subject. the angle the ray takes is defined by phi, as is the angle between a line from the camera position to the center of the subject.}}{34}
\contentsline {figure}{\numberline {4.2}{\ignorespaces In this figure, the left hand side shows a ratio of areas covered by the two parameterizations, with the two plane area in the numerator. It is clear from this that the smaller the distance between the cameras and the subject, the more appropriate it is to use a spherical parameterization.}}{35}
\contentsline {figure}{\numberline {4.3}{\ignorespaces This is an example of the typical error comparison. The image on top is the original image that is one of the ten percent of images removed from the light field. It is compared with the novel image created in its original location.}}{36}
\contentsline {figure}{\numberline {4.4}{\ignorespaces The top image shows the L2 error for each percentage of images removed from the light field. The bottom image shows the increase in the standard deviation as a function of the percentage of images removed. Interestingly, the error mean increases exponentially, while the error standard deviation seems to increase linearly.}}{37}
\contentsline {figure}{\numberline {4.5}{\ignorespaces This shows the shape of the error curve for all of the images at a particular percentage (in this case, ten percent of the images have been removed from the light field). It most closely matches a log normal curve.}}{38}
\contentsline {figure}{\numberline {4.6}{\ignorespaces In this figure, the robots z directional error is shown from horizontal (y direction) movements.}}{38}
\contentsline {figure}{\numberline {4.7}{\ignorespaces The top images shows the y directional error when the robot is moving left. The bottom images shows the y directional error when the robot is moving right.}}{39}
